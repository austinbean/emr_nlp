{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network\n### Austin Bean\n### 06/20/2019\n# nn notes\n\n<!-- idea: write in JMD to produce PDF via command below, but convert to ipynb to make executable notebook. -->\n<!-- in principle notebook may be more important so probably worth writing that first... -->\n<!-- see: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html -->\n<!--from within the directory: weave(joinpath(pwd(), \"nn_notes.jmd\"), out_path=:pwd, doctype=\"md2pdf\")-->\n<!-- convert to ipynb: convert_doc(\"nn_notes.jmd\", \"nn_notes.ipynb\") -->\n<!-- http://weavejl.mpastell.com/stable/notebooks/#Output-to-Jupyter-notebooks-1 -->\n<!--\nThis can run within the Jupyter environment after conversion using the function above\nRequires:\n- install anaconda\n- install IJulia\n- from within Julia, run:\nusing IJulia\nnotebook()\n\nnote that the weave command should be run from a terminal, not within Atom.\n\nJulia markdown syntax:\nhttps://docs.julialang.org/en/v1/stdlib/Markdown/#Markdown-1\n\nWeave extension:\nhttp://weavejl.mpastell.com/stable/publish/\n(under supported markdown syntax)\n-->\n\n\n- Word embeddings from word2vec\n- Train RNN before this or after?\n- Which features, in other words\n\n## import some labeled data\n\n- Word embeddings come from word2vec\n- Embeddings are organized in the file train_embedding.jl\n- Labels come from a Stata file\n- Import data, pull out\n\n\nFirst load the data"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Flux, CSVFiles, DataFrames, Word2Vec\n\ndat1 = DataFrame(load(\"embedded_data.csv\"));"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now separate the labels:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "target = convert(Array{Float64,1}, dat1[:x1])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And load the inputs - in this case, the longest diet sentence is 22 words long, so there are at most 22\ncolumns in this data."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "input = convert(Array{Float64,2}, dat1[:,[:x2, :x3, :x4, :x5, :x6, :x7, :x8, :x9, :x10, :x11, :x12, :x13, :x14, :x15, :x16, :x17, :x18, :x19, :x20, :x21, :x22]])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the vector of word embeddings:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "embed_1 = wordvectors(\"./diet_embed\");\n\nget_vector(embed_1, \"NEOSURE\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now setup the RNN.\n- Bi-directional RNN is best, I think.\n- This should take each (word, state) as an input and output a (state) to be fed back in.\n<!-- https://colah.github.io/posts/2015-08-Understanding-LSTMs/ -->\n\n<!-- https://github.com/maetshju/flux-blstm-implementation/blob/master/01-blstm.jl -->\n\n## Recurrent RNN from Goldberg:\n\n- We are given an input $x_{1}, â€¦, x_{n}$.  Each input is represented by a vector $x_{i} \\in \\mathbb{R}^{d_{in}}$\n- In my case the inputs are the words.  Each word has a single-dimensional input $\\mathbb{R}$ from the embeddings\ngiven by word2vec, though this need not be the case more generally. Single sentences are stored in the `input` table above.\n- The RNN maps the vector of inputs $x_{1}, \\ldots, x_{n}$ to the output $y_{n} \\in \\mathbb{R}^{d_{out}}$\n$$\ny_{1:n} = RNN^* (x_{1:n})\n$$\n$$\ny_{i} = RNN(x_{1:i})\n$$\n$$\nx_{i} \\in \\mathbb{R}^{d_{in}}, y_{i} \\in \\mathbb{R}^{d_{out}}\n$$\n- Think of $y_{1:i}$ as a different output for each possible $i=1, \\ldots, n$.\n- $y_{1:n}=RNN^* (x_{1:n})$ represents the entire sequence of outputs $y_{1:1}, y_{1:2}, y_{1:3}, \\ldots$\n\nFeatures in this case are given very easily by the row of the `input` table, but this does not capture the\nfact that the set of features input to the model might be growing.\n\n\n# Implementation\n\nNow define some functions."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\n    `inp_vec(x, i; bi=false)`\n    this function takes a set of features and returns input vector.\n    Can do bidirectional as well w/ `bi = true`.\n    Returns a one-dimensional vector of embeddings of words either up to `i`\n    or from `1:i` and then `i:end` (including `i` twice).\n\"\"\"\nfunction inp_vec(x, i; bi=false)\n  if bi\n    return vcat(copy(x[1:i]), reverse(x[i:end]))\n  else\n    return copy(x[1:i])\n  end\nend"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.1.0"
    },
    "kernelspec": {
      "name": "julia-1.1",
      "display_name": "Julia 1.1.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
